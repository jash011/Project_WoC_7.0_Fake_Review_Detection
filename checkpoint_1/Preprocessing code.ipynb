{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuxHmxllTwN"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## Preprocessing code\n",
        "</div>\n",
        "\n",
        "Fake review detector. Pre processed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldf6vfl5YrJ3",
        "outputId": "f5c63590-014c-44a8-a7c7-4ca4cbaf0efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40432 entries, 0 to 40431\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   category  40432 non-null  object \n",
            " 1   rating    40432 non-null  float64\n",
            " 2   label     40432 non-null  object \n",
            " 3   text_     40432 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 1.2+ MB\n",
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...   None \n",
            "--------\n",
            "Rows containing NULL values category    0\n",
            "rating      0\n",
            "label       0\n",
            "text_       0\n",
            "dtype: int64\n",
            "\n",
            "--------\n",
            "Duplicate value containing rows 12\n",
            "\n",
            "--------\n",
            "Data Scan Completed\n",
            "\n",
            "Duplicate Data Removed\n",
            "\n",
            "Removed Rows containing NULL values\n",
            "\n",
            "Removed Rows containing short reviews\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 40420 entries, 0 to 40431\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   category  40420 non-null  object \n",
            " 1   rating    40420 non-null  float64\n",
            " 2   label     40420 non-null  object \n",
            " 3   text_     40420 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 1.5+ MB\n",
            "None\n",
            "\n",
            "Duplicates: 0\n",
            "\n",
            "                                                text_  \\\n",
            "0  Love this!  Well made, sturdy, and very comfor...   \n",
            "1  love it, a great upgrade from the original.  I...   \n",
            "2  This pillow saved my back. I love the look and...   \n",
            "3  Missing information on how to use it, but it i...   \n",
            "4  Very nice set. Good quality. We have had the s...   \n",
            "\n",
            "                                            nrm_text  \n",
            "0  love this   well made  sturdy  and very comfor...  \n",
            "1  love it  a great upgrade from the original   i...  \n",
            "2  this pillow saved my back  i love the look and...  \n",
            "3  missing information on how to use it  but it i...  \n",
            "4  very nice set  good quality  we have had the s...  \n",
            "\n",
            "Data Normalized\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "tokanization Complete\n",
            "\n",
            "                                                text_  \\\n",
            "0  Love this!  Well made, sturdy, and very comfor...   \n",
            "1  love it, a great upgrade from the original.  I...   \n",
            "2  This pillow saved my back. I love the look and...   \n",
            "3  Missing information on how to use it, but it i...   \n",
            "4  Very nice set. Good quality. We have had the s...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, this, well, made, sturdy, and, very, co...  \n",
            "1  [love, it, a, great, upgrade, from, the, origi...  \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...  \n",
            "3  [missing, information, on, how, to, use, it, b...  \n",
            "4  [very, nice, set, good, quality, we, have, had...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stopwords Removed\n",
            "\n",
            "                                               tokens  \\\n",
            "0  [love, this, well, made, sturdy, and, very, co...   \n",
            "1  [love, it, a, great, upgrade, from, the, origi...   \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...   \n",
            "3  [missing, information, on, how, to, use, it, b...   \n",
            "4  [very, nice, set, good, quality, we, have, had...   \n",
            "\n",
            "                                         filt_tokens  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lemmatized tokens\n",
            "\n",
            "                                          filt_tokens  \\\n",
            "0  [love, well, made, sturdy, comfortable, love, ...   \n",
            "1  [love, great, upgrade, original, mine, couple,...   \n",
            "2    [pillow, saved, back, love, look, feel, pillow]   \n",
            "3  [missing, information, use, great, product, pr...   \n",
            "4       [nice, set, good, quality, set, two, months]   \n",
            "\n",
            "                                        lemma_tokens  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4        [nice, set, good, quality, set, two, month]  \n",
            "\n",
            "                                         lemma_tokens  \\\n",
            "0  [love, well, made, sturdy, comfortable, love, ...   \n",
            "1  [love, great, upgrade, original, mine, couple,...   \n",
            "2    [pillow, saved, back, love, look, feel, pillow]   \n",
            "3  [missing, information, use, great, product, pr...   \n",
            "4        [nice, set, good, quality, set, two, month]   \n",
            "\n",
            "                                      final_text  \n",
            "0  love well made sturdy comfortable love pretty  \n",
            "1   love great upgrade original mine couple year  \n",
            "2        pillow saved back love look feel pillow  \n",
            "3    missing information use great product price  \n",
            "4            nice set good quality set two month  \n",
            "pre done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40420/40420 [00:00<00:00, 40662.69it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#review data file loading\n",
        "df = pd.read_csv('/content/fakeReviewData.csv')\n",
        "\n",
        "# Check the data structure\n",
        "print(df.head(), df.info(),\"\\n--------\")\n",
        "print(\"Rows containing NULL values\",df.isnull().sum())\n",
        "print (\"\\n--------\")\n",
        "print (\"Duplicate value containing rows\", df.duplicated().sum())\n",
        "print (\"\\n--------\")\n",
        "print(\"Data Scan Completed\")\n",
        "\n",
        "# Data cleaning\n",
        "df = df.drop_duplicates()\n",
        "print(\"\\nDuplicate Data Removed\")\n",
        "\n",
        "df = df.dropna(subset=['text_'])\n",
        "print(\"\\nRemoved Rows containing NULL values\")\n",
        "\n",
        "df = df[df['text_'].str.len() > 10]\n",
        "print(\"\\nRemoved Rows containing short reviews\\n\")\n",
        "\n",
        "print(df.info())\n",
        "print(\"\\nDuplicates:\", df.duplicated().sum())\n",
        "\n",
        "\n",
        "# Data Normalization\n",
        "def normalize_text(text): # function to make lowercse and remove numbers,special char,punctuation\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    return text\n",
        "\n",
        "df['nrm_text'] = df['text_'].apply(normalize_text) # aplly funtion to all text\n",
        "\n",
        "print(\"\\n\",df[['text_', 'nrm_text']].head()) # comparision\n",
        "\n",
        "print(\"\\nData Normalized\\n\")\n",
        "\n",
        "# tokanization of data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # pretrained data\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df['tokens'] = df['nrm_text'].apply(word_tokenize)\n",
        "print(\"\\ntokanization Complete\")\n",
        "print(\"\\n\",df[['text_', 'tokens']].head()) # comparision\n",
        "\n",
        "\n",
        "# Stopword removal\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "df['filt_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "print(\"\\nStopwords Removed\")\n",
        "print(\"\\n\",df[['tokens', 'filt_tokens']].head()) # comparision\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemma_tokens'] = df['filt_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "print(\"\\nLemmatized tokens\")\n",
        "print(\"\\n\",df[['filt_tokens','lemma_tokens']].head()) # comparision\n",
        "\n",
        "#Vectorize output\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "df['final_text'] = df['lemma_tokens'].apply(lambda tokens: ' '.join(tokens))  # Convert list to text\n",
        "print(\"\\n\",df[['lemma_tokens','final_text']].head()) # comparision\n",
        "\n",
        "df.to_csv(\"preprocessed_reviews.csv\", index=False)\n",
        "print(\"pre done\")\n",
        "\n",
        "X = vectorizer.fit_transform(tqdm(df['final_text']))  # Convert text to numerical vectors\n",
        "vec_matrix =pd.DataFrame.sparse.from_spmatrix(X,columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "vec_matrix.to_csv('tfidf_matrix.csv', index=False)\n",
        "print(\"pre processed data ready\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
